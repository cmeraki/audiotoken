{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405d852d-f861-4cb2-90a7-c949b51e07d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from vector_quantize_pytorch import VectorQuantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84f04c0-3b01-4815-a921-a293c0d478c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c9c469-107e-44ff-be65-dacf7ce4ac80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m1 = torch.load('../data/vq_hubert_60k_run5/quanitzer__L11_C2048_ckpt11000.pkl', map_location='cuda:0')\n",
    "m2 = torch.load('../data/vq_w2vbert2_60k_run1/quantizer__L19_C2048_ckpt62500.pkl', map_location='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb3a312-c0ed-4f93-8e41-8407860b1b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vq = VectorQuantize(\n",
    "    dim=1024,\n",
    "    codebook_size=2048,\n",
    "    decay=0.8,\n",
    "    commitment_weight=1\n",
    ")\n",
    "vq.to(device)\n",
    "\n",
    "vq.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79294170-4689-46d1-984c-d15e336666a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state_dict = {}\n",
    "\n",
    "for k, v in m2.items():\n",
    "    new_state_dict[k] = v\n",
    "\n",
    "vq.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e2c221-a253-4e09-8dd9-e272dfee911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f14f85-9875-4657-952e-955f6db63a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Wav2Vec2FeatureExtractor, AutoFeatureExtractor, WhisperFeatureExtractor\n",
    "\n",
    "from src.utils import read_audio, find_files\n",
    "from src.encoder import Wav2VecBertEncoder, HubertEncoder, WhisperEncoder, hubert_processor, whisper_processor\n",
    "from src.configs import Wav2VecBertConfig, HubertEncoderConfig, WhisperEncoderConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c71ce14-5917-4983-a4f9-69ce40f52511",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HubertEncoderConfig.model_id = '../data/model/trimmed/hubert_11/'\n",
    "\n",
    "processor = Wav2Vec2FeatureExtractor.from_pretrained(HubertEncoderConfig.model_id)\n",
    "encoder = HubertEncoder(HubertEncoderConfig, quantize=False, compile=False, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad6d0fb-7bca-4229-9773-e747208575dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Wav2VecBertEncoder(\n",
    "    config=Wav2VecBertConfig(),\n",
    "    compile=False,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadc6ca4-2b41-4d05-9e94-b9f5cf64c460",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 100\n",
    "layer = 19\n",
    "\n",
    "audio_files = find_files('/home/meraki/projects/tmp/flatfiles/indicvoices/', '.wav')\n",
    "audio_files = np.random.choice(audio_files, samples, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7df978-2c47-4bd4-bf2e-82624edf15f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def get_vq_dist(embeddings: torch.Tensor, quant: torch.nn.Module) -> Tuple:\n",
    "    \"\"\"\n",
    "    Compute the distance between embeddings and centroids\n",
    "\n",
    "    Args:\n",
    "        embeddings (torch.Tensor): B, T, D\n",
    "        centroids (torch.Tensor): K, D\n",
    "\n",
    "    Returns:\n",
    "        Tuple: (Value, Indices): B, T\n",
    "    \"\"\"\n",
    "    # centroids, indices, commit_loss = quant(embeddings)\n",
    "    # print(commit_loss)\n",
    "    # distances = torch.cdist(embeddings, centroids)\n",
    "    centroids = quant._codebook.embed\n",
    "    distances = torch.cdist(embeddings, centroids)\n",
    "    \n",
    "    return torch.min(distances, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45336a4-2fcc-4ea3-9e08-c1bc310c79c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "audio_distances = []\n",
    "audio_tokens = []\n",
    "embeddings = []\n",
    "\n",
    "print(f'Computing embeddings')\n",
    "\n",
    "for a in tqdm(audio_files, total=samples):\n",
    "    audio = read_audio(a, 16_000)\n",
    "    audio = audio[:, :160_000]\n",
    "\n",
    "    ii = audio\n",
    "    am = torch.ones_like(ii)\n",
    "    ii = F.pad(ii, (0, 160_000-ii.shape[1]), value=0)\n",
    "    am = F.pad(am, (0, 160_000-am.shape[1]), value=0)\n",
    "\n",
    "    out = encoder(ii.to(device), am.to(device))\n",
    "    out = out[layer]\n",
    "    d = get_vq_dist(out, vq)\n",
    "\n",
    "    embeddings.append(out.cpu().numpy())\n",
    "    audio_distances.extend(d.values.cpu().numpy())\n",
    "    audio_tokens.extend(d.indices.cpu().numpy())\n",
    "\n",
    "seq_len, dim = embeddings[0].shape[1:]\n",
    "embeddings = torch.from_numpy(np.array(embeddings)).reshape(samples*seq_len, dim)\n",
    "audio_distances = np.array(audio_distances).reshape(-1, 1)\n",
    "audio_tokens = np.array(audio_tokens).reshape(-1, 1)\n",
    "\n",
    "print(f'Shape of embeddings: {embeddings.shape} and audio_distances: {audio_distances.shape} and audio_tokens: {audio_tokens.shape}')\n",
    "\n",
    "norms = torch.linalg.vector_norm(embeddings, dim=-1)\n",
    "\n",
    "random_embeddings = []\n",
    "random_distances = []\n",
    "random_tokens = []\n",
    "\n",
    "print(f'Generating random embeddings')\n",
    "\n",
    "for norm in tqdm(norms):\n",
    "    random_vec = torch.randn((1, dim))\n",
    "    random_vec = random_vec / torch.norm(random_vec)\n",
    "    random_vec = random_vec * norm\n",
    "    random_embeddings.append(random_vec)\n",
    "\n",
    "    d = get_vq_dist(random_vec.to(device), vq)\n",
    "\n",
    "    random_distances.append(d.values.detach().cpu().numpy())\n",
    "    random_tokens.append(d.indices.detach().cpu().numpy())\n",
    "\n",
    "random_distances = np.array(random_distances)\n",
    "random_tokens = np.array(random_tokens)\n",
    "\n",
    "print(f'Shape of random_embeddings: {len(random_embeddings)} and random_distances: {random_distances.shape} and random_tokens: {random_tokens.shape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85e8278-9fb9-4194-8b06-49e256df4402",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].hist(audio_distances, alpha=0.75, label='Audio Tokens')\n",
    "ax[0].hist(random_distances[:, :, 0], alpha=0.5, label='Random Tokens')\n",
    "ax[0].set_title('Histogram of Distances')\n",
    "ax[0].set_xlabel('Distance')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "ax[0].legend()\n",
    "\n",
    "# Plot the distribution of tokens across the centroids\n",
    "ax[1].hist(audio_tokens, bins=100, alpha=0.75, label='Audio Tokens')\n",
    "ax[1].hist(random_tokens[:, :, 0], bins=100, alpha=0.5, label='Random Tokens')\n",
    "ax[1].set_title('Histogram of Tokens')\n",
    "ax[1].set_xlabel('Token')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5d44b7-7b4b-4bc2-b644-a3d417e41790",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].hist(audio_distances, alpha=0.75, label='Audio Tokens')\n",
    "ax[0].hist(random_distances[:, :, 0], alpha=0.5, label='Random Tokens')\n",
    "ax[0].set_title('Histogram of Distances')\n",
    "ax[0].set_xlabel('Distance')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "ax[0].legend()\n",
    "\n",
    "# Plot the distribution of tokens across the centroids\n",
    "ax[1].hist(audio_tokens, bins=100, alpha=0.75, label='Audio Tokens')\n",
    "ax[1].hist(random_tokens[:, :, 0], bins=100, alpha=0.5, label='Random Tokens')\n",
    "ax[1].set_title('Histogram of Tokens')\n",
    "ax[1].set_xlabel('Token')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889d307f-75db-4f8b-9037-f27cf2c294d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ind = np.unique(audio_tokens, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7382242-a266-41cf-bb76-ac263571971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = np.unique(audio_tokens, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b59310-46cd-4376-893a-679a8a7e72d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(ind[0])\n",
    "plt.hist(ind[0],alpha=0.5, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb564092-2038-4423-a5e1-be38d8f39b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(ind[0])\n",
    "plt.hist(eng[0],alpha=0.5, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295c276b-6683-4bb1-b700-2806a5ee7ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.intersect1d(ind[0], eng[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064a6eef-f09c-4fe2-8d76-68113e10c84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671c2636-07e5-4634-857f-6cb0092c8ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37293dfb-4d93-46b3-917b-3301f6935059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "romit",
   "language": "python",
   "name": "romit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

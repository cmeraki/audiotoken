# indri-inference

Inference Service for Indri

This repository holds the inference service for the models in Indri. The service will be deployed as a web app, where the user will be able to write some text and select from a list of voices to speak from. The user will then be able to hear the text in the voice selected.

There are 3 models end to end for the complete inference. This service should be able to handle concurrent requests and scale up based on traffic. A high-level design is as follows:

![inference](assets/indri-inference.png)

Figure: HLD of the service. AR = Auto Regressive, NAR = Non-auto Regressive

These are some design choices (host: CPU, device: GPU):

1. The text tokenizer will run on the host so that we can avoid transferring raw text to the device. We will only transfer integer token IDs to the device
2. The voice prompts will be tokenized beforehand and the tokens for all the voice prompts will reside in the device's memory. This way, whenever a voice is selected by the user, the service will only send the ID of the voice to the device
3. All the 3 models will ideally run on separate devices so that they can independently scale up and manage their batches
   1. This can change if coarse to fine model is very small and the network cost is greater than the inference cost
4. The final audio tokens generated by the coarse to fine model will be sent back to the host for it to be decoded
5. The text tokenizer and the voice decoder will work in a batched manner on a pre-defined memory layout. This way, there will be no dynamic memory allocation after run-time
6. All 3 models will be powered by vLLM backend and infer in a batched manner

Open questions

1. Do we allow batches to be modified while the previous batch is in progress?
2. Ideal batch sizes for all the models including tokenizer and voice decoder

# indri-inference

Inference Service for Indri

This repository holds the inference service for the models in Indri. The service will be deployed as a web app, where the user will be able to write some text and select from a list of voices to speak from. The user will then be able to hear the text in the voice selected.

There are 3 models end to end for the complete inference. This service should be able to handle concurrent requests and scale up based on traffic. A high-level design is as follows:

![inference](assets/indri-inference.png)

Figure: HLD of the service. AR = Auto Regressive, NAR = Non-auto Regressive

These are some design choices (host: CPU, device: GPU):

1. The text tokenizer will run on the host so that we can avoid transferring raw text to the device. We will only transfer integer token IDs to the device
2. The voice prompts will be tokenized beforehand and the tokens for all the voice prompts will reside in the device's memory. This way, whenever a voice is selected by the user, the service will only send the ID of the voice to the device
3. All the 3 models will ideally run on separate devices so that they can independently scale up and manage their batches
   1. This can change if coarse to fine model is very small and the network cost is greater than the inference cost
4. The final audio tokens generated by the coarse to fine model will be sent back to the host for it to be decoded
5. The text tokenizer and the voice decoder will work in a batched manner on a pre-defined memory layout. This way, there will be no dynamic memory allocation after run-time
6. All 3 models will be powered by vLLM backend and infer in a batched manner

Open questions

1. Do we allow batches to be modified while the previous batch is in progress?
2. Ideal batch sizes for all the models including tokenizer and voice decoder

---

1. [Week 1] Implement Encoder and Decoder classes supporting fixed-size batches
   1. Test against naive implementation
2. [Week 2 and 3] vLLM backend for text-to-semantic model
   1. Only takes tokens as input
   2. Verify the tokens generated by this model - How?
3. vLLM backend for semantic-to-coarse model
   1. Takes tokens from the previous model
   2. Outputs coarse tokens - Verify generated tokens by this model - How?
4. [Week 4 and 5] vLLM backend for coarse-to-fine model
   1. How to do unrolling?
   2. Most efficient implementation? - Need to integrate to vLLM or some other way is efficient?
   3. How to train a more efficient model here?
5. [Week 6] Testing of E2E pipeline

---

## Encodec with better batching

We implement `encodec` with batching. There are two ways to implement batching

1. [Naive batching] Take a batch of audio files, pad it to the maximum length of the audio in the batch and pass it to the model
2. [Fixed batching] Take a batch of audio files, split each audio file into smaller chunks and pass the batch of the chunk to the model. This will lead to minimum wastage of resources and ensure that you don't get OOM errors. This implementation accepts the following parameters:
   1. 

We tested both the implementations on CPU and GPU using the librispeech-test-clean dataset which is 5.4 hours long. For our setup, we noticed the following metrics:

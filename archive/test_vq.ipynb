{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405d852d-f861-4cb2-90a7-c949b51e07d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from vector_quantize_pytorch import VectorQuantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84f04c0-3b01-4815-a921-a293c0d478c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c9c469-107e-44ff-be65-dacf7ce4ac80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# m1 = torch.load('../data/vq_hubert_60k_run5/quanitzer__L11_C2048_ckpt11000.pkl', map_location='cuda:0')\n",
    "m2 = torch.load('../data/vq_w2vbert_mix_run2/quantizer__L19_C2048_ckpt9000.pkl', map_location='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb3a312-c0ed-4f93-8e41-8407860b1b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vq = VectorQuantize(\n",
    "    dim=1024,\n",
    "    codebook_size=2048,\n",
    "    decay=0.8,\n",
    "    commitment_weight=1\n",
    ")\n",
    "vq.to(device)\n",
    "\n",
    "vq.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79294170-4689-46d1-984c-d15e336666a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state_dict = {}\n",
    "\n",
    "for k, v in m2.items():\n",
    "    new_state_dict[k] = v\n",
    "\n",
    "vq.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e2c221-a253-4e09-8dd9-e272dfee911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f14f85-9875-4657-952e-955f6db63a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Wav2Vec2FeatureExtractor, AutoFeatureExtractor, WhisperFeatureExtractor\n",
    "\n",
    "from src.utils import read_audio, find_files\n",
    "from src.encoder import Wav2VecBertEncoder, HubertEncoder, WhisperEncoder, hubert_processor, whisper_processor\n",
    "from src.configs import Wav2VecBertConfig, HubertEncoderConfig, WhisperEncoderConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c71ce14-5917-4983-a4f9-69ce40f52511",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HubertEncoderConfig.model_id = '../data/model/trimmed/hubert_11/'\n",
    "\n",
    "processor = Wav2Vec2FeatureExtractor.from_pretrained(HubertEncoderConfig.model_id)\n",
    "encoder = HubertEncoder(HubertEncoderConfig, quantize=False, compile=False, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad6d0fb-7bca-4229-9773-e747208575dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Wav2VecBertEncoder(\n",
    "    config=Wav2VecBertConfig(),\n",
    "    compile=False,\n",
    "    device=device,\n",
    "    quantize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadc6ca4-2b41-4d05-9e94-b9f5cf64c460",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 100\n",
    "layer = 19\n",
    "\n",
    "audio_files = find_files('/home/meraki/projects/tmp/flatfiles/gigaspeech/', '.opus')\n",
    "audio_files = np.random.choice(audio_files, samples, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7df978-2c47-4bd4-bf2e-82624edf15f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def get_vq_dist(embeddings: torch.Tensor, quant: torch.nn.Module) -> Tuple:\n",
    "    \"\"\"\n",
    "    Compute the distance between embeddings and centroids\n",
    "\n",
    "    Args:\n",
    "        embeddings (torch.Tensor): B, T, D\n",
    "        centroids (torch.Tensor): K, D\n",
    "\n",
    "    Returns:\n",
    "        Tuple: (Value, Indices): B, T\n",
    "    \"\"\"\n",
    "    # centroids, indices, commit_loss = quant(embeddings)\n",
    "    # print(commit_loss)\n",
    "    # distances = torch.cdist(embeddings, centroids)\n",
    "    centroids = quant._codebook.embed\n",
    "    distances = torch.cdist(embeddings, centroids)\n",
    "    \n",
    "    return torch.min(distances, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45336a4-2fcc-4ea3-9e08-c1bc310c79c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "audio_distances = []\n",
    "audio_tokens = []\n",
    "embeddings = []\n",
    "\n",
    "print(f'Computing embeddings')\n",
    "\n",
    "for a in tqdm(audio_files, total=samples):\n",
    "    audio = read_audio(a, 16_000)\n",
    "    audio = audio[:, :160_000]\n",
    "\n",
    "    ii = audio\n",
    "    am = torch.ones_like(ii)\n",
    "    ii = F.pad(ii, (0, 160_000-ii.shape[1]), value=0)\n",
    "    am = F.pad(am, (0, 160_000-am.shape[1]), value=0)\n",
    "\n",
    "    out = encoder(ii.to(device), am.to(device))\n",
    "    out = out[layer]\n",
    "    d = get_vq_dist(out, vq)\n",
    "\n",
    "    embeddings.append(out.cpu().numpy())\n",
    "    audio_distances.extend(d.values.cpu().numpy())\n",
    "    audio_tokens.extend(d.indices.cpu().numpy())\n",
    "\n",
    "seq_len, dim = embeddings[0].shape[1:]\n",
    "embeddings = torch.from_numpy(np.array(embeddings)).reshape(samples*seq_len, dim)\n",
    "audio_distances = np.array(audio_distances).reshape(-1, 1)\n",
    "audio_tokens = np.array(audio_tokens).reshape(-1, 1)\n",
    "\n",
    "print(f'Shape of embeddings: {embeddings.shape} and audio_distances: {audio_distances.shape} and audio_tokens: {audio_tokens.shape}')\n",
    "\n",
    "norms = torch.linalg.vector_norm(embeddings, dim=-1)\n",
    "\n",
    "random_embeddings = []\n",
    "random_distances = []\n",
    "random_tokens = []\n",
    "\n",
    "print(f'Generating random embeddings')\n",
    "\n",
    "for norm in tqdm(norms):\n",
    "    random_vec = torch.randn((1, dim))\n",
    "    random_vec = random_vec / torch.norm(random_vec)\n",
    "    random_vec = random_vec * norm\n",
    "    random_embeddings.append(random_vec)\n",
    "\n",
    "    d = get_vq_dist(random_vec.to(device), vq)\n",
    "\n",
    "    random_distances.append(d.values.detach().cpu().numpy())\n",
    "    random_tokens.append(d.indices.detach().cpu().numpy())\n",
    "\n",
    "random_distances = np.array(random_distances)\n",
    "random_tokens = np.array(random_tokens)\n",
    "\n",
    "print(f'Shape of random_embeddings: {len(random_embeddings)} and random_distances: {random_distances.shape} and random_tokens: {random_tokens.shape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85e8278-9fb9-4194-8b06-49e256df4402",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].hist(audio_distances, alpha=0.75, label='Audio Tokens')\n",
    "ax[0].hist(random_distances[:, :, 0], alpha=0.5, label='Random Tokens')\n",
    "ax[0].set_title('Histogram of Distances')\n",
    "ax[0].set_xlabel('Distance')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "ax[0].legend()\n",
    "\n",
    "# Plot the distribution of tokens across the centroids\n",
    "ax[1].hist(audio_tokens, bins=100, alpha=0.75, label='Audio Tokens')\n",
    "ax[1].hist(random_tokens[:, :, 0], bins=100, alpha=0.5, label='Random Tokens')\n",
    "ax[1].set_title('Histogram of Tokens')\n",
    "ax[1].set_xlabel('Token')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5d44b7-7b4b-4bc2-b644-a3d417e41790",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].hist(audio_distances, alpha=0.75, label='Audio Tokens')\n",
    "ax[0].hist(random_distances[:, :, 0], alpha=0.5, label='Random Tokens')\n",
    "ax[0].set_title('Histogram of Distances')\n",
    "ax[0].set_xlabel('Distance')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "ax[0].legend()\n",
    "\n",
    "# Plot the distribution of tokens across the centroids\n",
    "ax[1].hist(audio_tokens, bins=100, alpha=0.75, label='Audio Tokens')\n",
    "ax[1].hist(random_tokens[:, :, 0], bins=100, alpha=0.5, label='Random Tokens')\n",
    "ax[1].set_title('Histogram of Tokens')\n",
    "ax[1].set_xlabel('Token')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889d307f-75db-4f8b-9037-f27cf2c294d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ind = np.unique(audio_tokens, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7382242-a266-41cf-bb76-ac263571971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = np.unique(audio_tokens, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b59310-46cd-4376-893a-679a8a7e72d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(ind[0])\n",
    "plt.hist(ind[0],alpha=0.5, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb564092-2038-4423-a5e1-be38d8f39b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(ind[0])\n",
    "plt.hist(eng[0],alpha=0.5, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f4fc38-5de8-4f5c-a339-2910bb357026",
   "metadata": {},
   "source": [
    "Change of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61baa2f-52ab-4f5d-bec6-5af3628e1d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from vector_quantize_pytorch import VectorQuantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d431d05-50f4-4a67-ba54-b4d79bc43942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.utils import read_audio, find_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17344784-c4fb-47e6-8b3f-e0d742732362",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc432759-32a7-462e-8f06-37bf49a4aac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vq = VectorQuantize(\n",
    "    dim=1024,\n",
    "    codebook_size=2048,\n",
    "    decay=0.8,\n",
    "    commitment_weight=1\n",
    ")\n",
    "vq.to(device)\n",
    "\n",
    "vq.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c917639-4677-4fca-96f9-f079da3609c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = vq(torch.randn(12, 500, 1024, device='cuda:1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023a078e-6aa6-498d-b8ec-254ffe83938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a921ee50-dbfa-429a-b53e-9c2633139f77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embeds = {}\n",
    "\n",
    "model_list = find_files('../data/vq_w2vbert_mix/', ('.pkl'))\n",
    "\n",
    "for model_path in model_list:\n",
    "    raw_model = torch.load(model_path, map_location=device)\n",
    "    ckpt = model_path.split('/')[-1].split('.pkl')[0].split('ckpt')[-1]\n",
    "    ckpt = int(ckpt)\n",
    "\n",
    "    embeds[ckpt] = raw_model['_codebook.embed'].unsqueeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9eb477-5955-441c-ac19-64be959577f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37293dfb-4d93-46b3-917b-3301f6935059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pairwise_distances(checkpoint_embeddings):\n",
    "    # Convert the dictionary to a list of tensors\n",
    "    embeddings = list(checkpoint_embeddings.values())\n",
    "    \n",
    "    # Stack the embeddings into a single tensor\n",
    "    embeddings_tensor = torch.stack(embeddings)\n",
    "    \n",
    "    # Compute pairwise distances\n",
    "    distances = torch.cdist(embeddings_tensor, embeddings_tensor, p=2)  # Euclidean distance\n",
    "    # print(distances.shape)\n",
    "    \n",
    "    # Create a dictionary to store the results\n",
    "    pairwise_distances = {}\n",
    "    checkpoints = list(checkpoint_embeddings.keys())\n",
    "    \n",
    "    # Populate the dictionary with pairwise distances\n",
    "    for i, cp1 in enumerate(checkpoints):\n",
    "        for j, cp2 in enumerate(checkpoints):\n",
    "            if i < j:  # To avoid redundant computations\n",
    "                pairwise_distances[(cp1, cp2)] = distances[i, j].item()\n",
    "    \n",
    "    return pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b83aa4b-baef-499d-81a3-78c0e7f0e058",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3faef59-2505-46a1-ae47-746a9d22f531",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_pairwise_distances(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d3972f-08be-4e5a-9b13-642618921235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "romit",
   "language": "python",
   "name": "romit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
